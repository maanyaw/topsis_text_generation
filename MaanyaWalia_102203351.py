# -*- coding: utf-8 -*-
"""MaanyaWalia_102203351.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z6fmOYqOb5fwtNESyC2hAWknjT9lMaGb
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler

def topsis(decision_matrix, weights, benefit_criteria):
    decision_matrix = np.array(decision_matrix, dtype=np.float64)
    weights = np.array(weights, dtype=np.float64)

    # Step 1-Normalizing the decision matrix using vector normalization
    norm_matrix = decision_matrix / np.sqrt((decision_matrix ** 2).sum(axis=0))

    # Step 2-Multiplying by weights
    weighted_matrix = norm_matrix * weights

    # Step 3-Identifying ideal and negative-ideal solutions
    ideal_best = np.max(weighted_matrix, axis=0) if benefit_criteria else np.min(weighted_matrix, axis=0)
    ideal_worst = np.min(weighted_matrix, axis=0) if benefit_criteria else np.max(weighted_matrix, axis=0)

    # Step 4-Calculating distances to ideal and negative-ideal solutions
    dist_best = np.sqrt(((weighted_matrix - ideal_best) ** 2).sum(axis=1))
    dist_worst = np.sqrt(((weighted_matrix - ideal_worst) ** 2).sum(axis=1))

    # Step 5-Calculating relative closeness
    closeness_score = dist_worst / (dist_best + dist_worst)

    # Step 6-Ranking models
    ranks = np.argsort(closeness_score)[::-1] + 1
    return closeness_score, ranks

# Sample data-models and criteria
models = ["GPT-4", "LLaMA-2", "Falcon", "Mistral", "Gemini"]
criteria = ["BLEU Score", "Perplexity", "Inference Speed", "Token Efficiency", "Cost"]

data = np.array([
    [85, 12, 50, 90, 70],  # GPT-4
    [80, 15, 55, 85, 60],  # LLaMA-2
    [75, 18, 60, 80, 50],  # Falcon
    [78, 14, 58, 83, 55],  # Mistral
    [82, 16, 54, 88, 65]   # Gemini
])

# Defining weights
weights = [0.3, 0.2, 0.2, 0.2, 0.1]  # Adjust based on priority

# Definining benefit criteria: Higher is better for BLEU, Speed, Token Efficiency; Lower is better for Perplexity, Cost
benefit_criteria = [True, False, True, True, False]

# Applying TOPSIS
scores, rankings = topsis(data, weights, benefit_criteria)

# Creating DataFrame for results
results_df = pd.DataFrame({"Model": models, "TOPSIS Score": scores, "Rank": rankings})
results_df = results_df.sort_values(by="Rank")

# Saving results as CSV and Markdown
results_df.to_csv("topsis_results.csv", index=False)
results_df.to_markdown("topsis_results.md", index=False)

# Ploting the results
plt.figure(figsize=(8, 5))
plt.barh(results_df["Model"], results_df["TOPSIS Score"], color='skyblue')
plt.xlabel("TOPSIS Score")
plt.ylabel("Model")
plt.title("Ranking of Pretrained Models Using TOPSIS")
plt.gca().invert_yaxis()
plt.savefig("topsis_ranking.png")
plt.show()

# Generate README.md for GitHub
github_readme = """# TOPSIS Analysis for Text Generation Models

This repository contains the analysis of different pretrained text generation models using the TOPSIS method.

## Overview
TOPSIS (Technique for Order Preference by Similarity to Ideal Solution) is a multi-criteria decision-making technique that helps in ranking models based on various performance metrics.

## Models Evaluated
- GPT-4
- LLaMA-2
- Falcon
- Mistral
- Gemini

## Criteria Used
- BLEU Score (Higher is better)
- Perplexity (Lower is better)
- Inference Speed (Higher is better)
- Token Efficiency (Higher is better)
- Cost (Lower is better)

## Results
### Ranked Models
```markdown
{results_df.to_markdown(index=False)}
```

![Ranking Chart](topsis_ranking.png)

## Files in Repository
- `MaanyaWalia_102203351.py` - Python script to perform TOPSIS analysis
- `topsis_results.csv` - Results in CSV format
- `topsis_results.md` - Results in Markdown format
- `topsis_ranking.png` - Bar chart visualization
- `README.md` - Project description for GitHub

## How to Run
1. Install dependencies: `pip install numpy pandas matplotlib`
2. Run the script: `python topsis_analysis.py`

"""
with open("README.md", "w") as f:
    f.write(github_readme)

print("Analysis complete. Results and README file saved.")